{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/deepset-ai/haystack.git\n",
      "  Cloning https://github.com/deepset-ai/haystack.git to /tmp/pip-req-build-izk0eu7q\n",
      "Requirement already satisfied (use --upgrade to upgrade): farm-haystack==0.5.0 from git+https://github.com/deepset-ai/haystack.git in /opt/conda/lib/python3.7/site-packages\n",
      "Requirement already satisfied: farm==0.5.0 in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.5.0)\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.61.2)\n",
      "Requirement already satisfied: uvicorn in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.12.3)\n",
      "Requirement already satisfied: gunicorn in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (20.0.4)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (1.1.4)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.0)\n",
      "Requirement already satisfied: elasticsearch<=7.10,>=7.7 in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (7.10.0)\n",
      "Requirement already satisfied: elastic-apm in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (5.10.0)\n",
      "Requirement already satisfied: tox in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (3.20.1)\n",
      "Requirement already satisfied: coverage in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (5.3)\n",
      "Requirement already satisfied: langdetect in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (1.0.8)\n",
      "Requirement already satisfied: python-multipart in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.0.5)\n",
      "Requirement already satisfied: python-docx in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.8.10)\n",
      "Requirement already satisfied: sqlalchemy_utils in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.36.8)\n",
      "Requirement already satisfied: tika in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (1.24)\n",
      "Requirement already satisfied: httptools in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.1.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (3.2.4)\n",
      "Requirement already satisfied: more_itertools in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (8.2.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (2.4)\n",
      "Requirement already satisfied: psycopg2-binary in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (2.8.6)\n",
      "Requirement already satisfied: faiss-cpu==1.6.3 in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (1.6.3)\n",
      "Requirement already satisfied: uvloop in /opt/conda/lib/python3.7/site-packages (from farm-haystack==0.5.0) (0.14.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (46.1.3.post20200325)\n",
      "Requirement already satisfied: flask-restplus in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (0.13.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (5.7.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (4.45.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (2.23.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.4.1)\n",
      "Requirement already satisfied: mlflow==1.0.0 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.0.0)\n",
      "Requirement already satisfied: Werkzeug==0.16.1 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (0.16.1)\n",
      "Requirement already satisfied: flask-cors in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (3.0.9)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (0.3.3)\n",
      "Requirement already satisfied: dotmap==1.3.0 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.3.0)\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.16.19)\n",
      "Requirement already satisfied: torch<1.7,>1.5 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.6.0)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (0.34.2)\n",
      "Requirement already satisfied: seqeval==0.0.12 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (0.0.12)\n",
      "Requirement already satisfied: transformers==3.3.1 in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (3.3.1)\n",
      "Requirement already satisfied: flask in /opt/conda/lib/python3.7/site-packages (from farm==0.5.0->farm-haystack==0.5.0) (1.1.2)\n",
      "Requirement already satisfied: starlette==0.13.6 in /opt/conda/lib/python3.7/site-packages (from fastapi->farm-haystack==0.5.0) (0.13.6)\n",
      "Requirement already satisfied: pydantic<2.0.0,>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from fastapi->farm-haystack==0.5.0) (1.7.2)\n",
      "Requirement already satisfied: click==7.* in /opt/conda/lib/python3.7/site-packages (from uvicorn->farm-haystack==0.5.0) (7.1.1)\n",
      "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from uvicorn->farm-haystack==0.5.0) (3.7.4.1)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.7/site-packages (from uvicorn->farm-haystack==0.5.0) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.7/site-packages (from pandas->farm-haystack==0.5.0) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->farm-haystack==0.5.0) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in /opt/conda/lib/python3.7/site-packages (from pandas->farm-haystack==0.5.0) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from sklearn->farm-haystack==0.5.0) (0.23.2)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.5.0) (2020.11.8)\n",
      "Requirement already satisfied: urllib3<2,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from elasticsearch<=7.10,>=7.7->farm-haystack==0.5.0) (1.25.9)\n",
      "Requirement already satisfied: virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (20.2.1)\n",
      "Requirement already satisfied: packaging>=14 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (20.1)\n",
      "Requirement already satisfied: pluggy>=0.12.0 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (0.13.0)\n",
      "Requirement already satisfied: filelock>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (3.0.10)\n",
      "Requirement already satisfied: py>=1.4.17 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (1.8.1)\n",
      "Requirement already satisfied: importlib-metadata<3,>=0.12; python_version < \"3.8\" in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (1.6.0)\n",
      "Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (1.14.0)\n",
      "Requirement already satisfied: toml>=0.9.4 in /opt/conda/lib/python3.7/site-packages (from tox->farm-haystack==0.5.0) (0.10.0)\n",
      "Requirement already satisfied: lxml>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from python-docx->farm-haystack==0.5.0) (4.5.0)\n",
      "Requirement already satisfied: SQLAlchemy>=1.0 in /opt/conda/lib/python3.7/site-packages (from sqlalchemy_utils->farm-haystack==0.5.0) (1.3.16)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.7/site-packages (from networkx->farm-haystack==0.5.0) (4.4.2)\n",
      "Requirement already satisfied: aniso8601>=0.82 in /opt/conda/lib/python3.7/site-packages (from flask-restplus->farm==0.5.0->farm-haystack==0.5.0) (8.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.7/site-packages (from flask-restplus->farm==0.5.0->farm-haystack==0.5.0) (3.2.0)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests->farm==0.5.0->farm-haystack==0.5.0) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->farm==0.5.0->farm-haystack==0.5.0) (2.9)\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (1.6.0)\n",
      "Requirement already satisfied: entrypoints in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (0.3)\n",
      "Requirement already satisfied: databricks-cli>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (0.14.1)\n",
      "Requirement already satisfied: simplejson in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (3.17.0)\n",
      "Requirement already satisfied: alembic in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (1.4.3)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (3.14.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (5.3.1)\n",
      "Requirement already satisfied: querystring-parser in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (1.2.4)\n",
      "Requirement already satisfied: docker>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (4.2.0)\n",
      "Requirement already satisfied: gitpython>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (3.1.1)\n",
      "Requirement already satisfied: sqlparse in /opt/conda/lib/python3.7/site-packages (from mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (0.3.1)\n",
      "Requirement already satisfied: botocore<1.20.0,>=1.19.19 in /opt/conda/lib/python3.7/site-packages (from boto3->farm==0.5.0->farm-haystack==0.5.0) (1.19.19)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from boto3->farm==0.5.0->farm-haystack==0.5.0) (0.3.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3->farm==0.5.0->farm-haystack==0.5.0) (0.10.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<1.7,>1.5->farm==0.5.0->farm-haystack==0.5.0) (0.18.2)\n",
      "Requirement already satisfied: Keras>=2.2.4 in /opt/conda/lib/python3.7/site-packages (from seqeval==0.0.12->farm==0.5.0->farm-haystack==0.5.0) (2.4.3)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.7/site-packages (from transformers==3.3.1->farm==0.5.0->farm-haystack==0.5.0) (0.1.94)\n",
      "Requirement already satisfied: tokenizers==0.8.1.rc2 in /opt/conda/lib/python3.7/site-packages (from transformers==3.3.1->farm==0.5.0->farm-haystack==0.5.0) (0.8.1rc2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers==3.3.1->farm==0.5.0->farm-haystack==0.5.0) (2020.4.4)\n",
      "Requirement already satisfied: sacremoses in /opt/conda/lib/python3.7/site-packages (from transformers==3.3.1->farm==0.5.0->farm-haystack==0.5.0) (0.0.43)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.7/site-packages (from flask->farm==0.5.0->farm-haystack==0.5.0) (2.11.2)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.7/site-packages (from flask->farm==0.5.0->farm-haystack==0.5.0) (1.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn->farm-haystack==0.5.0) (0.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->sklearn->farm-haystack==0.5.0) (2.1.0)\n",
      "Requirement already satisfied: distlib<1,>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox->farm-haystack==0.5.0) (0.3.1)\n",
      "Requirement already satisfied: appdirs<2,>=1.4.3 in /opt/conda/lib/python3.7/site-packages (from virtualenv!=20.0.0,!=20.0.1,!=20.0.2,!=20.0.3,!=20.0.4,!=20.0.5,!=20.0.6,!=20.0.7,>=16.0.0->tox->farm-haystack==0.5.0) (1.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=14->tox->farm-haystack==0.5.0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata<3,>=0.12; python_version < \"3.8\"->tox->farm-haystack==0.5.0) (3.1.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->flask-restplus->farm==0.5.0->farm-haystack==0.5.0) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema->flask-restplus->farm==0.5.0->farm-haystack==0.5.0) (19.3.0)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.7/site-packages (from databricks-cli>=0.8.0->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (0.8.7)\n",
      "Requirement already satisfied: python-editor>=0.3 in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (1.0.4)\n",
      "Requirement already satisfied: Mako in /opt/conda/lib/python3.7/site-packages (from alembic->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (1.1.3)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker>=3.6.0->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (0.57.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (4.0.4)\n",
      "Requirement already satisfied: h5py in /opt/conda/lib/python3.7/site-packages (from Keras>=2.2.4->seqeval==0.0.12->farm==0.5.0->farm-haystack==0.5.0) (2.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask->farm==0.5.0->farm-haystack==0.5.0) (1.1.1)\n",
      "Requirement already satisfied: smmap<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->gitpython>=2.1.0->mlflow==1.0.0->farm==0.5.0->farm-haystack==0.5.0) (3.0.2)\n",
      "Building wheels for collected packages: farm-haystack\n",
      "  Building wheel for farm-haystack (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for farm-haystack: filename=farm_haystack-0.5.0-py3-none-any.whl size=101538 sha256=3d6b7e5c44e977202365507600697972b974223d44dab008fd2c790315c43ac5\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-g3nfzkl5/wheels/a7/05/3b/9b33368d9af06a39f8e6af2e97fa2af876e893ade323cfc2c9\n",
      "Successfully built farm-haystack\n"
     ]
    }
   ],
   "source": [
    "# installing haystack\n",
    "! pip install git+https://github.com/deepset-ai/haystack.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.6.2-linux-x86_64.tar.gz -q\n",
    "! tar -xzf elasticsearch-7.6.2-linux-x86_64.tar.gz\n",
    "! chown -R daemon:daemon elasticsearch-7.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# General libraries\n",
    "import re, os, string, random, requests\n",
    "import pandas as pd\n",
    "from subprocess import Popen, PIPE, STDOUT\n",
    "\n",
    "# Haystack importings\n",
    "from haystack import Finder\n",
    "from haystack.reader.farm import FARMReader\n",
    "from haystack.utils import print_answers\n",
    "from haystack.document_store.elasticsearch import ElasticsearchDocumentStore\n",
    "from haystack.retriever.sparse import ElasticsearchRetriever\n",
    "\n",
    "# Scikit-learn importings\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting ElasticSearch server as daemon\n",
    "es_server = Popen(['elasticsearch-7.6.2/bin/elasticsearch'],\n",
    "                   stdout=PIPE, stderr=STDOUT,\n",
    "                   preexec_fn=lambda: os.setuid(1)  # as daemon\n",
    "                  )\n",
    "\n",
    "# wait until ElasticSearch has started\n",
    "! sleep 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(n):\n",
    "    \"\"\"Return a random string of length n\"\"\"\n",
    "    letters = string.ascii_lowercase\n",
    "    result_str = ''.join(random.choice(letters) for i in range(n))\n",
    "    return result_str\n",
    "\n",
    "def get_stop_words(stop_file_path):\n",
    "    \"\"\"load stop words \"\"\"\n",
    "    \n",
    "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
    "        stopwords = f.readlines()\n",
    "        stop_set = set(m.strip() for m in stopwords)\n",
    "        return frozenset(stop_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_doc(doc):\n",
    "    \"\"\"Trim doc with respect to the boundary of a sentence.\"\"\"\n",
    "    \n",
    "    trimmedText = []\n",
    "    charCount = 0\n",
    "    for sentence in doc.split('.'):\n",
    "        if charCount < DOC_THRESHOLD:\n",
    "            charCount+=len(sentence.strip())\n",
    "            trimmedText.append(sentence)\n",
    "\n",
    "    finalText = \".\".join(trimmedText)\n",
    "    \n",
    "    return finalText\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Doc cleaning\"\"\"\n",
    "    \n",
    "    # Lowering text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Removing punctuation\n",
    "    text = \"\".join([c for c in text if c not in PUNCTUATION])\n",
    "    \n",
    "    # Removing whitespace and newlines\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    \n",
    "    # Trimming doc\n",
    "    text = trim_doc(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    \"\"\"Sort a dict with highest score\"\"\"\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "ES_INDEX = get_index(10) # Elastic Search DB index name\n",
    "PUNCTUATION = \"\"\"!\"#$%&'()*+,-/:;<=>?@[\\]^_`{|}~\"\"\" # excluding . (full-stop) from the set of punctuations\n",
    "DOC_THRESHOLD = 10000 # character limit for a doc\n",
    "TOP_K_RETRIEVER = 10 # top k documents to analyze further for a given query\n",
    "TOP_K_READER = 5 # top k number of answers to return\n",
    "TOP_K_KEYWORDS = 10 # top k number of keywords to retrieve in a ranked document\n",
    "BASE_URL = \"http://localhost:9200/\"+ES_INDEX+\"/_doc/\"\n",
    "STOPWORD_PATH = \"../input/ai-papers/data/stopwords.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and Its Applications</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-and-its-applications.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Arti...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-a...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Term Potentiation and Depression ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long-term-potentiation-and-depress...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\nLONG?TERM POTENTIATION AND DEP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network Models</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-network-models.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwork Models\\nGerhard Paass\\nJorg ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, and Active Learning</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation-and-active-learning.pdf</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year  \\\n",
       "0     1  1987   \n",
       "1    10  1987   \n",
       "2   100  1988   \n",
       "3  1000  1994   \n",
       "4  1001  1994   \n",
       "\n",
       "                                                                             title  \\\n",
       "0                   Self-Organization of Associative Database and Its Applications   \n",
       "1  A Mean Field Theory of Layer IV of Visual Cortex and Its Application to Arti...   \n",
       "2  Storing Covariance by the Associative Long-Term Potentiation and Depression ...   \n",
       "3                            Bayesian Query Construction for Neural Network Models   \n",
       "4                  Neural Network Ensembles, Cross Validation, and Active Learning   \n",
       "\n",
       "  event_type  \\\n",
       "0        NaN   \n",
       "1        NaN   \n",
       "2        NaN   \n",
       "3        NaN   \n",
       "4        NaN   \n",
       "\n",
       "                                                                          pdf_name  \\\n",
       "0             1-self-organization-of-associative-database-and-its-applications.pdf   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-cortex-and-its-application-to-a...   \n",
       "2  100-storing-covariance-by-the-associative-long-term-potentiation-and-depress...   \n",
       "3                   1000-bayesian-query-construction-for-neural-network-models.pdf   \n",
       "4           1001-neural-network-ensembles-cross-validation-and-active-learning.pdf   \n",
       "\n",
       "           abstract  \\\n",
       "0  Abstract Missing   \n",
       "1  Abstract Missing   \n",
       "2  Abstract Missing   \n",
       "3  Abstract Missing   \n",
       "4  Abstract Missing   \n",
       "\n",
       "                                                                        paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABASE\\nAND ITS APPLICATIONS\\nHisa...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISUAL CORTEX\\nAND ITS APPLICATION...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\nLONG?TERM POTENTIATION AND DEP...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwork Models\\nGerhard Paass\\nJorg ...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, and Active Learning\\n\\nAnders K...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"../input/nips-papers/papers.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7241, 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structuring data to haystack required format\n",
    "# Format: [{'text': 'paper_content', 'meta':{'name':'title'}}]\n",
    "docs = []\n",
    "corpora = []\n",
    "doc_len = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    dicts = {}\n",
    "    dicts['text'] = clean_text(row['paper_text'])\n",
    "    doc_len.append(len(dicts['text']))\n",
    "    corpora.append(dicts['text'])\n",
    "    dicts['meta'] = {}\n",
    "    dicts['meta']['name'] = clean_text(row['title'])\n",
    "    docs.append(dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10245.576577820742"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average characters in a document after trimming\n",
    "sum(doc_len)/len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful while overwriting data on the same ES index\n",
    "document_store = ElasticsearchDocumentStore(host=\"localhost\", username=\"\", password=\"\", index=ES_INDEX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's write the dicts containing documents to our DB.\n",
    "document_store.write_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiating ES retriever \n",
    "retriever = ElasticsearchRetriever(document_store=document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initializing reader on the top of roberta-base-squad2 pre-trained model, which will be downloaded on the first run\n",
    "# Here, we can set the size of context window for our answers and use the GPU if available\n",
    "\n",
    "reader = FARMReader(model_name_or_path=\"ahotrod/albert_xxlargev1_squad2_512\",use_gpu=True, context_window_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting reader and retriever to Finder\n",
    "finder = Finder(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.73s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.43s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.70s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.19s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.70s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.60s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.61s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.43s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.60s/ Batches]\n",
      "Inferencing Samples: 100%|██████████| 1/1 [00:02<00:00,  2.60s/ Batches]\n"
     ]
    }
   ],
   "source": [
    "# Question prediction with TOP_K_RETRIEVER and TOP_K_READER\n",
    "question = \"What is the use of CNN?\"\n",
    "prediction = finder.get_answers(question=question, top_k_retriever=TOP_K_RETRIEVER, top_k_reader=TOP_K_READER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   {   'answer': 'to extract semantic vectors',\n",
      "        'context': 'there are also questions requiring extra knowledge or '\n",
      "                   'commonsense reasoning such as does it appear to be rainy. '\n",
      "                   'properly modeling questions is essential for solving the '\n",
      "                   'vqa problem. a commonly employed strategy is to use a cnn '\n",
      "                   'or an rnn to extract semantic vectors. the general issue '\n",
      "                   'is that the resulting question representation lacks '\n",
      "                   'detailed information from the given image which however is '\n",
      "                   'vital for understanding visual content. we take the '\n",
      "                   'question and image in figure 1 as an example. to answ'},\n",
      "    {   'answer': 'to extract the visual representation',\n",
      "        'context': 'stions about the content of an image. the answer can be a '\n",
      "                   'sentence a phrase or a single word. our model contains '\n",
      "                   'four components a long shortterm memory lstm to extract '\n",
      "                   'the question representation a convolutional neural network '\n",
      "                   'cnn to extract the visual representation an lstm for '\n",
      "                   'storing the linguistic context in an answer and a fusing '\n",
      "                   'component to combine the information from the first three '\n",
      "                   'components and generate the answer. we construct a '\n",
      "                   'freestyle multilingual image question answering fmiqa'},\n",
      "    {   'answer': 'text categorization',\n",
      "        'context': ' classifier as features for classification. here we '\n",
      "                   'focused on the convolution layer for other details 11 '\n",
      "                   'should be consulted. 2 semisupervised cnn with '\n",
      "                   'tvembeddings for text categorization it was shown in 11 '\n",
      "                   'that onehot cnn is effective on text categorization where '\n",
      "                   'the essence is direct learning of an embedding of text '\n",
      "                   'regions aided by new options of input region vector '\n",
      "                   'representation. we go further along this line and propose '\n",
      "                   'a semisupervised learning framework that learns an '\n",
      "                   'embedding of text'},\n",
      "    {   'answer': 'to handle strong noise',\n",
      "        'context': 'ae structure 21 is a good choice for denoise and '\n",
      "                   'inpainting. agostinelli et al. 22 generalized it by '\n",
      "                   'combining multiple sdae for handling different types of '\n",
      "                   'noise. in 23 and 16 the convolutional neural network cnn '\n",
      "                   'architecture 24 was used to handle strong noise such as '\n",
      "                   'raindrop and lens dirt. schuler et al. 13 added mlps to a '\n",
      "                   'direct deconvolution to remove artifacts. though the '\n",
      "                   'network structure works well for denoise it does not work '\n",
      "                   'similarly for deconvolution. how to adapt the architecture '\n",
      "                   'is'},\n",
      "    {   'answer': 'to extract image features',\n",
      "        'context': 't combines natural language processing with semantic '\n",
      "                   'segmentation in a bayesian framework for automatic '\n",
      "                   'question answering. since it several neural network based '\n",
      "                   'models 16 19 2 were proposed to solve the vqa problem. '\n",
      "                   'these models use cnn to extract image features and '\n",
      "                   'recurrent neural networks to embed questions. the embedded '\n",
      "                   'image and question features are then fused by '\n",
      "                   'concatenation 16 2 image understanding region 1 image '\n",
      "                   'question what are they playing 1 query 1 query gru query 1 '\n",
      "                   '2 query 1 soft'}]\n"
     ]
    }
   ],
   "source": [
    "# Printing answers with minimal detail\n",
    "# details = minimal | medium | all\n",
    "\n",
    "print_answers(prediction, details=\"minimal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5_docs = []\n",
    "\n",
    "for doc in prediction['answers']:\n",
    "    DOC_URL = BASE_URL + doc['document_id']\n",
    "    response = requests.get(DOC_URL)\n",
    "    if response.status_code == 200:\n",
    "        full_doc = {}\n",
    "        full_doc['title'] = response.json()['_source']['name']\n",
    "        full_doc['text'] = response.json()['_source']['text']\n",
    "        full_doc['answer'] = doc['answer']\n",
    "        top_5_docs.append(full_doc)\n",
    "    else:\n",
    "        print(\"Document not found! Restart the ES Server\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Top K keywords using TF-IDF Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['come', 'vis', 'viser', 'visest'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "#load a set of stop words\n",
    "stopwords=get_stop_words(STOPWORD_PATH)\n",
    "\n",
    "# Initializing TF-IDF Vectorizer with stopwords\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords, smooth_idf=True, use_idf=True)\n",
    "\n",
    "# Creating vocab with our corpora\n",
    "vectorizer.fit_transform(corpora)\n",
    "\n",
    "# Storing vocab\n",
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(vectorizer, feature_names, doc):\n",
    "    \"\"\"Return top k keywords from a doc using TF-IDF method\"\"\"\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector = vectorizer.transform([doc])\n",
    "    \n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only TOP_K_KEYWORDS\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,TOP_K_KEYWORDS)\n",
    "    \n",
    "    return list(keywords.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in top_5_docs:\n",
    "    doc['keywords'] = get_keywords(vectorizer, feature_names, doc['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(top_5_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the use of CNN?\n",
      "Top 5 articles with keywords\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>answer</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>visual question answering with question representation update qru</td>\n",
       "      <td>visual question answering with question representation update qru ruiyu li j...</td>\n",
       "      <td>to extract semantic vectors</td>\n",
       "      <td>[vqa, image, question, answering, reasoning, questions, regions, reasoner, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>are you talking to a machine dataset and methods for multilingual image ques...</td>\n",
       "      <td>are you talking to a machine dataset and methods for multilingual image ques...</td>\n",
       "      <td>to extract the visual representation</td>\n",
       "      <td>[mqa, answer, question, dataset, image, lstm, fmiqa, freestyle, questionansw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>semisupervised convolutional neural networks for text categorization via reg...</td>\n",
       "      <td>semisupervised convolutional neural networks for text categorization via reg...</td>\n",
       "      <td>text categorization</td>\n",
       "      <td>[cnn, text, tvembedding, embedding, region, onehot, word, unlabeled, regions...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>deep convolutional neural network for image deconvolution</td>\n",
       "      <td>deep convolutional neural network for image deconvolution li xu lenovo resea...</td>\n",
       "      <td>to handle strong noise</td>\n",
       "      <td>[deconvolution, image, denoise, artifacts, blur, ssdae, degradation, cnn, ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>visual question answering with question representation update qru</td>\n",
       "      <td>visual question answering with question representation update qru ruiyu li j...</td>\n",
       "      <td>to extract image features</td>\n",
       "      <td>[vqa, image, question, answering, reasoning, questions, regions, reasoner, c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             title  \\\n",
       "0                visual question answering with question representation update qru   \n",
       "1  are you talking to a machine dataset and methods for multilingual image ques...   \n",
       "2  semisupervised convolutional neural networks for text categorization via reg...   \n",
       "3                        deep convolutional neural network for image deconvolution   \n",
       "4                visual question answering with question representation update qru   \n",
       "\n",
       "                                                                              text  \\\n",
       "0  visual question answering with question representation update qru ruiyu li j...   \n",
       "1  are you talking to a machine dataset and methods for multilingual image ques...   \n",
       "2  semisupervised convolutional neural networks for text categorization via reg...   \n",
       "3  deep convolutional neural network for image deconvolution li xu lenovo resea...   \n",
       "4  visual question answering with question representation update qru ruiyu li j...   \n",
       "\n",
       "                                 answer  \\\n",
       "0           to extract semantic vectors   \n",
       "1  to extract the visual representation   \n",
       "2                   text categorization   \n",
       "3                to handle strong noise   \n",
       "4             to extract image features   \n",
       "\n",
       "                                                                          keywords  \n",
       "0  [vqa, image, question, answering, reasoning, questions, regions, reasoner, c...  \n",
       "1  [mqa, answer, question, dataset, image, lstm, fmiqa, freestyle, questionansw...  \n",
       "2  [cnn, text, tvembedding, embedding, region, onehot, word, unlabeled, regions...  \n",
       "3  [deconvolution, image, denoise, artifacts, blur, ssdae, degradation, cnn, ne...  \n",
       "4  [vqa, image, question, answering, reasoning, questions, regions, reasoner, c...  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(question)\n",
    "print(\"Top 5 articles with keywords\\n\")\n",
    "final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
